{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Conversational RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a73fbf-e241-499d-a235-32a87b46ee7c",
   "metadata": {},
   "source": [
    "## Intro\n",
    "* In most RAG applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ef906-9484-4b29-a963-e64e9ba68d3b",
   "metadata": {},
   "source": [
    "## The problem\n",
    "* How do we handle when the user refers to previous Q&As in the conversation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ddb93-56cf-4f09-941e-16ea1d0812e3",
   "metadata": {},
   "source": [
    "## The second problem...\n",
    "* This is probably the topic that is worst explained in the LangChain documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e3967-d201-458f-adbe-c27c104e2eed",
   "metadata": {},
   "source": [
    "## What we need to solve\n",
    "* Store the chat conversation.\n",
    "* When the user enters a new input, put that input in context.\n",
    "* Re-phrase the user input to have a contextualized input.\n",
    "* Send the contextualized input to the retriever.\n",
    "* Use the retriever to build a conversational rag chain.\n",
    "* Add extra features like persising memory (save memory in a file) and session memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d049b49-4776-4ec4-aec5-02483ec25910",
   "metadata": {},
   "source": [
    "## The process we will follow\n",
    "1. Create a basic RAG without memory.\n",
    "2. Create a ChatPrompTemplate able to contextualize inputs.\n",
    "3. Create a retriever aware of memory.\n",
    "4. Create a basic conversational RAG.\n",
    "5. Create an advanced conversational RAG with persistence and session memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 001-conversational-rag.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a7245-c5fe-4c4e-98e7-31b4708e8698",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-conversational-rag**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436a426-8b00-43f1-b523-60534a289f44",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4955a96-5e55-4d55-b92e-bf50538be227",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033a1517-e787-4002-817c-7737e18a2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "## Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c4cd4-2855-4708-9c94-6dfc4b25bff9",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f22855-1656-49e8-b1d9-a352affbd814",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a7e19-cf0b-4ffe-96d8-181be96ac986",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc942a2e-b9d3-4f5f-a612-f212cacc8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community langchain-chroma bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbec79-d2de-4d55-a2fa-e2125f914e56",
   "metadata": {},
   "source": [
    "## The process we will follow\n",
    "1. Create a basic RAG without memory.\n",
    "2. Create a ChatPrompTemplate able to contextualize inputs.\n",
    "3. Create a retriever aware of memory.\n",
    "4. Create a basic conversational RAG.\n",
    "5. Create an advanced conversational RAG with persistence and session memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f596c-4904-49b2-ad65-7c10988d5bf5",
   "metadata": {},
   "source": [
    "## Step 1: Create a basic RAG without memory\n",
    "* We will use the RAG process we already know.\n",
    "* We will use create_stuff_documents_chain to build a qa chain: a chain able to asks questions to an LLM.\n",
    "* We will use create_retrieval_chain and the qa chain to build the RAG chain: a chain able to asks questions to the retriever and then format the response with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f29f71-3210-47b4-91d2-82ae30d11c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "#from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"./data/be-good.txt\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97dff6a-e703-4ed9-9701-3e08cbaabb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616863f-6b53-40f3-bcd1-bc0dd73c5207",
   "metadata": {},
   "source": [
    "* Let's try the app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7772399b-c563-4a46-9051-2c66344c632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rag_chain.invoke({\"input\": \"What is this article about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b471ef7-7723-4b24-9769-27381ada7dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the motto \"Make something people want\" coined by Y Combinator founders and how it relates to building successful businesses. It explores the idea of focusing on creating value for users before worrying about monetization, suggesting that this approach could resemble a charity model. Examples like Craigslist are used to illustrate this concept of running a successful business with a focus on user needs over profit.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804378d-7639-451e-95a8-b40f1419e907",
   "metadata": {},
   "source": [
    "* As we can see in the following question, our app has no memory of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34cc6fb8-0778-4df4-880f-d4957ff90c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rag_chain.invoke({\"input\": \"What was my previous question about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6660b23a-4203-44c8-a516-84bf9b233f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your previous question was about the concept of benevolence in businesses and organizations, specifically how being benevolent can lead to success and growth. The idea was discussed in relation to examples such as Google, Microsoft, and Craigslist, highlighting the potential power of benevolence as a guiding principle.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b495ecd-9dba-45fe-b192-f2210ee4f382",
   "metadata": {},
   "source": [
    "## Step 2: Create a ChatPromptTemplate able to contextualize inputs\n",
    "* Goal: put the input in context and re-phrase it so we have a contextualized input.\n",
    "* We will define a new system prompt that instructs the LLM in how to contextualize the input.\n",
    "* Our new ChatPromptTemplate will include:\n",
    "    * The new system prompt.\n",
    "    * MessagesPlaceholder, a placeholder used to pass the list of messages included in the chat_history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeb66712-913d-4943-831b-ea80aeda1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b770b5-da83-4687-a5b5-e01f34b0ee39",
   "metadata": {},
   "source": [
    "## Step 3: Create a Retriever aware of the memory\n",
    "* We will build our new retriever with create_history_aware_retriever that uses the contextualized input to get a contextualized response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74591ec2-2461-4a67-9187-82b3d052df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0aeb46-3002-407a-950f-bd07d6eb0fdc",
   "metadata": {},
   "source": [
    "## Step 4: Create a basic Conversational RAG\n",
    "* We will use the retriever aware of memory, that uses the prompt with contextualized input.\n",
    "* We will use create_stuff_documents_chain to build a qa chain: a chain able to asks questions to an LLM.\n",
    "* We will use create_retrieval_chain and the qa chain to build the RAG chain: a chain able to asks questions to the retriever and then format the response with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b154434-7bdd-48e8-9438-5839c2a186d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e119c-ece2-4686-af5b-1bdc1f1ea19f",
   "metadata": {},
   "source": [
    "#### Trying our basic conversational RAG\n",
    "Below we ask a question and a follow-up question that requires contextualization to return a sensible response. Because our chain includes a \"chat_history\" input, the caller needs to manage the chat history. We can achieve this by appending input and output messages to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4a1e69d-105f-492c-9b2f-c956522ababd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your previous question was about the topic or subject of the article under discussion.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is this article about?\"\n",
    "\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What was my previous question about?\"\n",
    "\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94384f26-b8ae-4e79-a275-7417dc296225",
   "metadata": {},
   "source": [
    "## Step 5: Advanced conversational RAG with persistence and session memories\n",
    "* We will store the chat history in a python dictionary. In advanced apps, you will use advanced ways to store chat history.\n",
    "* Associate chat history with user session with the function get_session_history().\n",
    "* Inject chat history into inputs and update it after each interaction using BaseChatMessageHistory and RunnableWithMessageHistory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c41fa-8f43-4da6-b3c7-20ec24f91aee",
   "metadata": {},
   "source": [
    "Let's now focus on how to handle and maintain chat history in the question and answer (Q&A) application to make conversations flow more naturally.\n",
    "\n",
    "Here’s a simplified explanation:\n",
    "\n",
    "1. **Stateful Management of Chat History**: Instead of manually entering previous parts of the conversation every time a new input is made, the application can automatically manage and update chat history. This means that the application remembers past interactions and uses that information to understand and respond to new questions more accurately.\n",
    "\n",
    "2. **Tools for Managing Chat History**:\n",
    "   - **BaseChatMessageHistory**: This component is responsible for storing the history of the conversation.\n",
    "   - **RunnableWithMessageHistory**: This acts as a wrapper around the main processing chain (LCEL chain) and the chat history storage (BaseChatMessageHistory). It takes care of adding the historical chat data to new inputs and updating the stored history after each response.\n",
    "\n",
    "3. **How It Works**: When you use these components in an application:\n",
    "   - The application automatically retrieves and updates the chat history every time it processes a new input. This helps in maintaining a continuous and coherent conversation flow.\n",
    "   - When using `RunnableWithMessageHistory`, it manages chat history using a configuration that includes a unique session identifier (`session_id`). This identifier helps the system know which conversation history to retrieve and update whenever a user interacts with the system.\n",
    "\n",
    "4. **Alternative ways to store the chat history**: In our simple implementation, chat histories might be stored in a basic dictionary. More complex systems might use databases like Redis to ensure more reliable and long-term storage of conversation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2173cc2c-859a-435f-92f8-071fd84e3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57be5f77-c58b-4910-86e5-76e3430634f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This article discusses the concept of creating something that people want as a motto for startup success. It explores the idea that focusing on making something people want, rather than worrying about the business model initially, can lead to success. The article suggests that businesses can benefit from operating more like charities in terms of meeting people's needs and achieving success.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is this article about?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"001\"}\n",
    "    },  # constructs a key \"001\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76537066-9b6a-49bf-bc23-665feacacd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your previous question was about the topic or subject of the article that was referenced in the provided text snippets.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What was my previous question about?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"001\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ece4c-a4a6-4f6e-8d9e-7c1b6a79c561",
   "metadata": {},
   "source": [
    "The conversation history can be inspected in the store dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1d8d41e-a2f1-4cc2-8833-c41e60d62e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is this article about?\n",
      "\n",
      "AI: This article discusses the concept of creating something that people want as a motto for startup success. It explores the idea that focusing on making something people want, rather than worrying about the business model initially, can lead to success. The article suggests that businesses can benefit from operating more like charities in terms of meeting people's needs and achieving success.\n",
      "\n",
      "User: What was my previous question about?\n",
      "\n",
      "AI: Your previous question was about the topic or subject of the article that was referenced in the provided text snippets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"001\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0037a-d5cb-4813-8671-bc914401575e",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 001-conversational-rag.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 001-conversational-rag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e055a2-eac5-43a2-8648-cd60bc3da61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
