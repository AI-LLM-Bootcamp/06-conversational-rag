{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Conversational RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a73fbf-e241-499d-a235-32a87b46ee7c",
   "metadata": {},
   "source": [
    "## Intro\n",
    "* In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### Recommended: create new virtualenv\n",
    "* mkdir your_project_name\n",
    "* cd your_project_name\n",
    "* pyenv virtualenv 3.11.4 your_venv_name\n",
    "* pyenv activate your_venv_name\n",
    "* pip install jupyterlab\n",
    "* jupyter lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f9e964-c5c2-4a13-b9ed-c449362ae7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "#### .env File\n",
    "Remember to include:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **conversationalRAG**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc942a2e-b9d3-4f5f-a612-f212cacc8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community langchainhub langchain-chroma bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f596c-4904-49b2-ad65-7c10988d5bf5",
   "metadata": {},
   "source": [
    "## Basic RAG without memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f29f71-3210-47b4-91d2-82ae30d11c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"./data/be-good.txt\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97dff6a-e703-4ed9-9701-3e08cbaabb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7772399b-c563-4a46-9051-2c66344c632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rag_chain.invoke({\"input\": \"What is this article about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b471ef7-7723-4b24-9769-27381ada7dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the concept of making something people want as a motto for startups, advising founders not to worry excessively about the business model initially. It explores the idea that creating something people want and not focusing too much on making money can resemble a charity. Examples like Craigslist running successfully with a charitable approach are also provided.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804378d-7639-451e-95a8-b40f1419e907",
   "metadata": {},
   "source": [
    "* As we can see in the following question, our app has no memory of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34cc6fb8-0778-4df4-880f-d4957ff90c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rag_chain.invoke({\"input\": \"What was my previous question about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6660b23a-4203-44c8-a516-84bf9b233f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your previous question was about the idea of betting against benevolence and how powerful forces like benevolence can lead to successful outcomes in various contexts, such as in the case of Internet startups like Google and Microsoft. The discussion also touched on the success of organizations like Craigslist that operate with a benevolent approach, even though they are not structured as charities.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ccd21-2c4c-4ebe-b27b-59daa0db8ba2",
   "metadata": {},
   "source": [
    "## Adding memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4093e5ea-9923-44cd-b982-d12e8c7f748f",
   "metadata": {},
   "source": [
    "We will now improve the application so that it better understands conversations over time, especially when questions relate to earlier parts of the conversation.\n",
    "\n",
    "1. **Chat History Context**: When you chat with someone, your questions might refer to things you discussed earlier. For example, if you ask, \"What are common ways of doing it?\" after discussing \"Task Decomposition,\" the system should understand that \"it\" means \"Task Decomposition.\"\n",
    "\n",
    "2. **Improving the App**: To handle this, the app needs two updates:\n",
    "   - **Prompt Update**: Change the app's input setup so it can use previous messages in the conversation to understand the context of new questions.\n",
    "   - **Contextualizing Questions**: Create a process where the app takes the latest question and, if necessary, reformulates it by using the conversation history. This helps the app understand what the question is actually about.\n",
    "\n",
    "3. **Technical Steps**:\n",
    "   - A new function is introduced to integrate previous messages into the app’s process when formulating responses.\n",
    "   - When a new question is asked, the app can look at the earlier conversation to better understand and answer the question. This involves taking the previous chat, analyzing it alongside the new question, and then reformulating the question if needed before looking for answers.\n",
    "\n",
    "This approach ensures the app responds more accurately to questions based on the full conversation, not just the last question asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeb66712-913d-4943-831b-ea80aeda1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5209b6-1cc8-439a-8264-41f0a6c30299",
   "metadata": {},
   "source": [
    "Let's now focus on how to enhance this question and answer (QA) system to better understand and use the context of an ongoing conversation when providing answers.\n",
    "\n",
    "Here's a breakdown of the process in simpler terms:\n",
    "\n",
    "1. **Rephrasing the Query**: Before looking up information (retrieving), the system reformulates the input question by considering the previous conversation. This helps ensure that the information it gathers is relevant to the current discussion.\n",
    "\n",
    "2. **Building the QA Chain**:\n",
    "   - **Updating the Retriever**: The system updates its information retrieval process to a new version that is aware of the conversation history. This new version is called `history_aware_retriever`.\n",
    "   - **Generating Answers**: A function named `create_stuff_documents_chain` is used to construct a `question_answer_chain`. This chain takes the rephrased query, the conversation history, and any relevant retrieved information to produce an answer.\n",
    "\n",
    "3. **Final Chain Assembly**:\n",
    "   - A final process chain, referred to as `rag_chain`, is created using another function called `create_retrieval_chain`.\n",
    "   - This chain first uses the `history_aware_retriever` to gather information relevant to the rephrased query and the conversation history.\n",
    "   - Then, the `question_answer_chain` uses this information along with the initial query and chat history to generate an answer.\n",
    "   - The system keeps track of all the intermediate information (like the retrieved context) to make the process more efficient and transparent.\n",
    "\n",
    "Essentially, the system enhances the accuracy and relevance of responses in a conversation by ensuring that every new question is considered in the full context of what has been previously discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b154434-7bdd-48e8-9438-5839c2a186d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e119c-ece2-4686-af5b-1bdc1f1ea19f",
   "metadata": {},
   "source": [
    "Below we ask a question and a follow-up question that requires contextualization to return a sensible response. Because our chain includes a \"chat_history\" input, the caller needs to manage the chat history. We can achieve this by appending input and output messages to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a1e69d-105f-492c-9b2f-c956522ababd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your previous question was about the general topic or theme of the article.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is this article about?\"\n",
    "\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What was my previous question about?\"\n",
    "\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c41fa-8f43-4da6-b3c7-20ec24f91aee",
   "metadata": {},
   "source": [
    "Let's now focus on how to handle and maintain chat history in the question and answer (Q&A) application to make conversations flow more naturally.\n",
    "\n",
    "Here’s a simplified explanation:\n",
    "\n",
    "1. **Stateful Management of Chat History**: Instead of manually entering previous parts of the conversation every time a new input is made, the application can automatically manage and update chat history. This means that the application remembers past interactions and uses that information to understand and respond to new questions more accurately.\n",
    "\n",
    "2. **Tools for Managing Chat History**:\n",
    "   - **BaseChatMessageHistory**: This component is responsible for storing the history of the conversation.\n",
    "   - **RunnableWithMessageHistory**: This acts as a wrapper around the main processing chain (LCEL chain) and the chat history storage (BaseChatMessageHistory). It takes care of adding the historical chat data to new inputs and updating the stored history after each response.\n",
    "\n",
    "3. **How It Works**: When you use these components in an application:\n",
    "   - The application automatically retrieves and updates the chat history every time it processes a new input. This helps in maintaining a continuous and coherent conversation flow.\n",
    "   - For example, when using `RunnableWithMessageHistory`, it manages chat history using a configuration that includes a unique session identifier (`session_id`). This identifier helps the system know which conversation history to retrieve and update whenever a user interacts with the system.\n",
    "\n",
    "4. **Example of Implementation**: In a simple implementation, chat histories might be stored in a basic dictionary. More complex systems might use databases like Redis to ensure more reliable and long-term storage of conversation data.\n",
    "\n",
    "Overall, these tools and methods help a Q&A application remember and utilize previous interactions, making the conversation feel more natural and informed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2173cc2c-859a-435f-92f8-071fd84e3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57be5f77-c58b-4910-86e5-76e3430634f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 2b2ecc13-4056-44e5-b5ea-9094a966498f not found for run baa1cdbf-9e8f-4dcd-af4e-e015efa8121c. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The article discusses the motto of Y Combinator, \"Make something people want,\" and how focusing on creating value for users can lead to success. It also explores the idea of running a business like a charity, using examples like Craigslist, and how this approach can still be successful. The author reflects on the surprising connection between creating something people want and operating like a charity in the business world.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is this article about?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"001\"}\n",
    "    },  # constructs a key \"001\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76537066-9b6a-49bf-bc23-665feacacd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 1b87cc43-334f-462d-b583-d9ec72795006 not found for run 92207bd3-623c-4d1d-b9af-9ce145f833bf. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your previous question was about the topic or subject matter of the article.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What was my previous question about?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"001\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ece4c-a4a6-4f6e-8d9e-7c1b6a79c561",
   "metadata": {},
   "source": [
    "The conversation history can be inspected in the store dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d8d41e-a2f1-4cc2-8833-c41e60d62e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is this article about?\n",
      "\n",
      "AI: The article discusses the motto of Y Combinator, \"Make something people want,\" and how focusing on creating value for users can lead to success. It also explores the idea of running a business like a charity, using examples like Craigslist, and how this approach can still be successful. The author reflects on the surprising connection between creating something people want and operating like a charity in the business world.\n",
      "\n",
      "User: What was my previous question about?\n",
      "\n",
      "AI: Your previous question was about the topic or subject matter of the article.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"001\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ca291-4c3f-47a6-8854-42d2ac953c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
